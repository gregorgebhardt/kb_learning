---
# DEFAULT
name: "DEFAULT"
repetitions: 4
iterations: 150
path: "data/trpo_ma"
gui: false

params: {}

env_config:
  !EvalEnv
  width: 1.
  height: 1.
  resolution: 600

  objects:
    - !ObjectConf
      idx: 0
      shape: square
      width: .05
      height: .05
      init: random

  kilobots: !KilobotsConf
    num: 10
    mean: random
    std: .03

---
name: "object_touching_embedding_type"
iterations: 200
repetitions: 4

params:
  sampling:
    reward_function: object_touching
    agent_reward: True
    swarm_reward: False
    num_objects: 4
    timesteps_per_batch: 2048
    done_after_steps: 2048

list:
  policy:
    swarm_net_type: ['mean', 'max']
    objects_net_type: ['mean', 'max']

---
name: "object_collecting_embedding_type"
iterations: 200
repetitions: 4

params:
  sampling:
    reward_function: object_collecting
    agent_reward: True
    swarm_reward: False
    num_objects: 4
    timesteps_per_batch: 2048
    done_after_steps: 2048

list:
  policy:
    swarm_net_type: ['mean', 'max']
    objects_net_type: ['mean', 'max']

---
name: velocity_agent_max_embeddings
iterations: 150
repetitions: 5

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleVelocityControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: max
    objects_net_type: max

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: velocity_agent_softmax_embeddings
iterations: 150
repetitions: 3

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleVelocityControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: softmax
    objects_net_type: softmax

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: acceleration_agent_max_embeddings
iterations: 150
repetitions: 5

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleAccelerationControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: max
    objects_net_type: max

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: acceleration_agent_softmax_embeddings
iterations: 150
repetitions: 3

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleAccelerationControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: softmax
    objects_net_type: softmax

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: "eval_num_objects"
iterations: 200

params:
  policy:
    swarm_net_size: [64]
    object_net_size: [64]
    extra_net_size: [64]
    concat_net_size: [64]

list:
  sampling:
    num_objects: [1, 2]

---
name: "eval_random_num_objects"
iterations: 100

params:
  sampling:
    num_objects: random

list:
  policy:
    concat_net_size: [[64], [128], [128, 128]]
