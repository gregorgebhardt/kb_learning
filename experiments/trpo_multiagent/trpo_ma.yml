---
# DEFAULT
name: "DEFAULT"
repetitions: 4
iterations: 150
path: "data/trpo_ma"
gui: false

params: {}

env_config:
  !EvalEnv
  width: 1.
  height: 1.
  resolution: 600

  objects:
    - !ObjectConf
      idx: 0
      shape: square
      width: .1
      height: .1
      init: random

  kilobots: !KilobotsConf
    num: 10
    mean: random
    std: .03

---
name: test
iterations: 250
repetitions: 5

params:
  sampling:
    reward_function: move_objects_to_center
    agent_type: SimpleVelocityControlKilobot
    num_objects: 4
    timesteps_per_batch: 4096
    done_after_steps: 256
  policy:
#    swarm_net_size: [64]
#    objects_net_size: [64]
    swarm_net_type: mean
    objects_net_type: max
#    extra_net_size: []
#    concat_net_size: [128, 64]
#    load_path: trpo_multiagent/policy.pkl
  trpo:
    gamma: .99
    lambda: .95

---
name: eval_move_objects_to_center
iterations: 250
repetitions: 5

params:
  sampling:
    reward_function: move_objects_to_center
    agent_type: SimpleVelocityControlKilobot
    num_objects: 4
    timesteps_per_batch: 5120
    done_after_steps: 512
  trpo:
    gamma: .99
    lambda: .95

list:
  policy:
    type:             ['swarm', 'swarm', 'swarm',   'swarm',   'mlp']
    swarm_net_type:   ['mean',  'max',   'softmax', 'softmax', '']
    objects_net_type: ['max',   'max',   'max',     'softmax', '']

---
name: eval_assembly
iterations: 200
repetitions: 10

params:
  sampling:
    reward_function: assembly
    agent_type: SimpleVelocityControlKilobot
    num_objects: 4
    timesteps_per_batch: 4096
    done_after_steps: 512
  trpo:
    gamma: .99
    lambda: .95

list:
  policy:
    type:             ['swarm', 'swarm', 'swarm',   'swarm',   'mlp']
    swarm_net_type:   ['mean',  'max',   'softmax', 'softmax', '']
    objects_net_type: ['max',   'max',   'max',     'softmax', '']

---
name: eval_embedding_type
iterations: 250
repetitions: 5

params:
  sampling:
    reward_function: moving_objects
    agent_type: SimpleVelocityControlKilobot
    num_objects: 4
    timesteps_per_batch: 4096
    done_after_steps: 1024
  trpo:
    gamma: .99
    lambda: .95

grid:
  policy:
    swarm_net_type: ['mean', 'max', 'softmax']
    objects_net_type: ['mean', 'max', 'softmax']

---
name: eval_mlp
iterations: 250
repetitions: 5

params:
  sampling:
    reward_function: moving_objects
    agent_type: SimpleVelocityControlKilobot
    num_objects: 4
    timesteps_per_batch: 4096
    done_after_steps: 1024
  policy:
    type: mlp
  trpo:
    gamma: .99
    lambda: .95

---
name: "object_touching_embedding_type"
iterations: 200
repetitions: 4

params:
  sampling:
    reward_function: object_touching
    agent_reward: True
    swarm_reward: False
    num_objects: 4
    timesteps_per_batch: 2048
    done_after_steps: 2048

list:
  policy:
    swarm_net_type: ['mean', 'max']
    objects_net_type: ['mean', 'max']

---
name: "object_collecting_embedding_type"
iterations: 200
repetitions: 4

params:
  sampling:
    reward_function: object_collecting
    agent_reward: True
    swarm_reward: False
    num_objects: 4
    timesteps_per_batch: 2048
    done_after_steps: 2048

list:
  policy:
    swarm_net_type: ['mean', 'max']
    objects_net_type: ['mean', 'max']

---
name: velocity_agent_max_embeddings
iterations: 150
repetitions: 5

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleVelocityControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: max
    objects_net_type: max
    concat_net_size:

list:
  policy:
    swarm_net_size: [[64], [128], [64, 64], [128, 128]]
    objects_net_size: [[64], [128], [64, 64], [128, 128]]

---
name: velocity_agent_softmax_embeddings
iterations: 150
repetitions: 3

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleVelocityControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: softmax
    objects_net_type: softmax

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: acceleration_agent_max_embeddings
iterations: 150
repetitions: 5

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleAccelerationControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: max
    objects_net_type: max

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: acceleration_agent_softmax_embeddings
iterations: 150
repetitions: 3

params:
  sampling:
    reward_function: object_cleanup_sparse
    agent_type: SimpleAccelerationControlKilobot
    num_objects: 10
    timesteps_per_batch: 2048  # 4096
    done_after_steps: 2048
  policy:
    swarm_net_type: softmax
    objects_net_type: softmax

list:
  policy:
    swarm_net_size: [[64], [128]]
    objects_net_size: [[64], [128]]

---
name: "eval_num_objects"
iterations: 200

params:
  policy:
    swarm_net_size: [64]
    object_net_size: [64]
    extra_net_size: [64]
    concat_net_size: [64]

list:
  sampling:
    num_objects: [1, 2]

---
name: "eval_random_num_objects"
iterations: 100

params:
  sampling:
    num_objects: random

list:
  policy:
    concat_net_size: [[64], [128], [128, 128]]
