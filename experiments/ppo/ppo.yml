---
# DEFAULT
name: "DEFAULT"
repetitions: 5
iterations: 350
path: "data/ppo"
gui: false

params: {}

env_config:
  !EvalEnv
  width: 1.2
  height: 1.2
  resolution: 600

  objects:
    - !ObjectConf
      idx: 0
      shape: square
      width: .15
      height: .15
      init: random
#    - !ObjectConf
#      idx: 1
#      shape: square
#      width: .15
#      height: .15
#      init: random
#    - !ObjectConf
#      idx: 2
#      shape: square
#      width: .15
#      height: .15
#      init: random
#    - !ObjectConf
#      idx: 3
#      shape: square
#      width: .15
#      height: .15
#      init: random

  light: !LightConf
    type: momentum
    radius: .2
    init: object

  kilobots: !KilobotsConf
    num: 10
    mean: light
    std: .03

---
name: eval_policy_type
iterations: 400
repetitions: 3

params:
  sampling:
    timesteps_per_batch: 2048
    done_after_steps: 256
    schedule: linear
  ppo:
    clip_range: 0.3
    gamma: .99
    lambda: .95
    schedule: constant
  policy:
    swarm_net_type: 'softmax'
    swarm_net_size: [64]
    objects_net_type: 'softmax'
    objects_net_size: [32]
    extra_net_size: [32]
    concat_net_size: [128, 64]
  buffer_len: 500

list:
  policy:
    type: ['mlp', 'swarm']

---
name: eval_entropy_penalty
iterations: 400
repetitions: 3

params:
  sampling:
    timesteps_per_batch: 2048
    done_after_steps: 256
    schedule: linear
  ppo:
    clip_range: 0.3
    gamma: .99
    lambda: .95
    schedule: constant
  policy:
    type: mlp
  buffer_len: 500

list:
  ppo:
    entropy_coefficient: [.0, .01, .02]

---
name: eval_timesteps_per_batch
iterations: 400
repetitions: 3

params:
  sampling:
    timesteps_per_batch: 2048
    done_after_steps: 256
    schedule: linear
  ppo:
    clip_range: 0.3
    gamma: .99
    lambda: .95
    schedule: constant
  policy:
    type: mlp
  buffer_len: 500

list:
  sampling:
    timesteps_per_batch: [1024, 2048, 4096]

---
name: test
repetitions: 5
iterations: 400

params:
  sampling:
    timesteps_per_batch: 1024  # 4096
    done_after_steps: 256
  ppo:
    clip_range: 0.3
    entropy_coefficient: .01
    gamma: .99
    lambda: .95
  policy:
    type: 'mlp'
    swarm_net_type: 'softmax'
    swarm_net_size: [64]
    objects_net_type: 'softmax'
    objects_net_size: [32]
    extra_net_size: [32]
    concat_net_size: [128, 64]
  buffer_len: 250

#list:
#  policy:
#    type: ['mlp', 'swarm']